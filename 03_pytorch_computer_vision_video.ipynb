{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvqQ+pKUueeLU8JPBfM7za",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viv-bad/pytorch-course/blob/master/03_pytorch_computer_vision_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Computer vision libraries in PyTorch\n",
        "\n",
        "* `torchvision` - base/main computer vision library in PyTorch\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computer vision\n",
        "* `torchvision.models` - pretrained computer vision models that you can leverage for your own problems\n",
        "* `torchvision.transforms` - functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.data.Dataset` - to create our own dataset with out own data, we have the base class for this in PyTorch\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1ffNp_vdGWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print (torch.__version__)\n",
        "print (torchvision.__version__)\n",
        "\n",
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "bFXrjZ_EkkFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", #where to download data to\n",
        "    train=True, # do we want the training dataset/testing dataset\n",
        "    download=True, #do we want to download it\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None, #do we want to transform the target data/labels\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\", #where to download data to\n",
        "    train=False, # do we want the training dataset/testing dataset\n",
        "    download=True, #do we want to download it\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None, #do we want to transform the target data/labels\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "00glpzCYmjtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "PvuJFRa5pr8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training example\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "PijyoxLgpv_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "PstsPXwop2ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx #we have 10 different items of clothing, so output shape must be 10"
      ],
      "metadata": {
        "id": "MA9iwOTAqE-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "EJz3-vLsqNWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our image\n",
        "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Image label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "OeZ7uTLXqSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualising our data"
      ],
      "metadata": {
        "id": "R6aIVConqZgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # matplotlib expects [height, width, colour_channel], so squeeze to get rid of colour channel dimension\n",
        "plt.title(f\"{label},  {class_names[label]}\")"
      ],
      "metadata": {
        "id": "uiZXnrM1rEw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"{label},  {class_names[label]}\")\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "rQfCwlwLrE1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "rows, cols = 4,4\n",
        "for i in range(1, rows*cols+1):\n",
        "  random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.title(f\"{label},  {class_names[label]}\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "i4eeInkirE4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think these items of clothing (images) could be modelled with pure linear lines? Or do we need non linearity"
      ],
      "metadata": {
        "id": "z9vnqlyirE7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our dataset into a Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches).\n",
        "\n",
        "Why?\n",
        "\n",
        "1. It is more computationally efficient, as in your comp hardware may not be able to look (store in memory) all the images in one hit. So break it down to 32 images at one time (batch_size=32).\n",
        "\n",
        "2. It gives our neural network more chances to update its gradients per epoch.\n"
      ],
      "metadata": {
        "id": "ZCnRlsJ-tRDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train dataset into DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # shuffle training data to remove order, so training data doesn't have order\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)# don't shuffle test/eval data, it's good to have the data in the same order. our model will never actually see the test data during training anyway\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "vlk0F6LbtZc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train_dataloader: {len(train_dataloader)}, batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test_dataloader: {len(test_dataloader)}, batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "9GBTkjQMtYu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside train dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "hOirDmKUtY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "kOYNARijtYxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of ML. modelling experiments, always start with a baseline model.\n",
        "\n",
        "A baseline model is a simple model you will try to improve upon with subsequent models/experiments.\n",
        "\n",
        "Start simple and add complexity when necessary."
      ],
      "metadata": {
        "id": "mc6yQh-JtRGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "x.shape\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [colour_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [colour_channels, height*width]\") # now after flattening we have one value per pixel (height*width)\n"
      ],
      "metadata": {
        "id": "Ryc7dSMctRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "YOaxApUerE-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set up model with input params\n",
        "\n",
        "model_0 = FashionMNISTModelV0(input_shape=784, #28x28\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names) #one for every class\n",
        "\n",
        "                              ).to(device)\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "tq7X7yd54Zkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1,1,28,28]).to(device)\n",
        "model_0(dummy_x) # output of logits"
      ],
      "metadata": {
        "id": "V6DFWPbJ4Z4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "UawCH-YK4Z7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function - since we're working with multiclass data, our loss function is `nn.CrossEntropyLoss()`\n",
        "* Optimizer - we will stick with `torch.optim.SGD()` (stochastic gradient descent)\n",
        "* Evaluation metric - since we're working on a classification problem, let's use accuracy as our evaluation metric.\n"
      ],
      "metadata": {
        "id": "yn_3sn1a4Z-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "nCJ9WC8B4aAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "Qh593ixg7Mm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a function to time our experiments\n",
        "\n",
        "Machine learning is very experimental.\n",
        "\n",
        "Two main things to track are:\n",
        "\n",
        "1. Model's performance (loss, accuracy etc)\n",
        "2. How fast it runs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENkqtY0a7Q34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "  \"\"\"\n",
        "  Prints difference between start and end time\"\"\"\n",
        "\n",
        "  total_time = end-start\n",
        "\n",
        "  print(f\"Train time on {device}: {total_time: .3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "9c0mKmE17RkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print_train_time(start=start_time, end=end_time, device=device)\n"
      ],
      "metadata": {
        "id": "BJH_3aGo7613"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creating a training loop and training a model on batches of data\n",
        "\n",
        "1. Loop through epochs\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate test loss *per batch*\n",
        "4. Print out what's happening\n",
        "5. Time is all"
      ],
      "metadata": {
        "id": "SgyYUVVP8Xpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for a progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# set seed and start timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set number of epochs (we'll keep it small for faster training time)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-----\")\n",
        "  ### Training\n",
        "  train_loss = 0\n",
        "  # Add loop to loop thorugh training batches\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model_0(X)\n",
        "\n",
        "    # 2. Calculate loss (per batch)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # accumulate train loss\n",
        "\n",
        "    # 3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4.  Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress only every 400 batches\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "      # 1. forward pass - FIXED: Using X_test instead of X\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. Calculate loss accumulatively\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "      # 3. Calculate accuracy\n",
        "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculate test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  # Print out\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
        "\n",
        "# Calculate training time - FIXED: Made device agnostic\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(\n",
        "    start=train_time_start_on_cpu,\n",
        "    end=train_time_end_on_cpu,\n",
        "    device=next(model_0.parameters()).device  # Get the actual device the model is on\n",
        ")"
      ],
      "metadata": {
        "id": "QUPY2Uz_8qsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Make predictions and get Model 0 results\n"
      ],
      "metadata": {
        "id": "CSFSic-jBOB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn, device):\n",
        "  \"\"\"Returns a dict containing the results of model predicting on data_loader\"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "\n",
        "  # Handle both tensor and float types appropriately\n",
        "  result = {\"model_name\": model.__class__.__name__}\n",
        "\n",
        "  # Check if loss is a tensor or float\n",
        "  if isinstance(loss, torch.Tensor):\n",
        "    result[\"model_loss\"] = loss.item()\n",
        "  else:\n",
        "    result[\"model_loss\"] = loss\n",
        "\n",
        "  # Check if acc is a tensor or float\n",
        "  if isinstance(acc, torch.Tensor):\n",
        "    result[\"model_acc\"] = acc.item()\n",
        "  else:\n",
        "    result[\"model_acc\"] = acc\n",
        "\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "zak-EMKxCzco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate model - results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device)\n",
        "\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "pKkbbTGSH0zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "yGwh9qJx8qzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modal 1: Building a better model with non-linearity\n",
        "\n",
        "We learned about the power of non-linearity in notebook 2."
      ],
      "metadata": {
        "id": "q2EYR4Nb8qv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "zhzLa1Oi8q2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_1 = FashionMNISTModelV1(input_shape=784, #28x28\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names) #one for every class\n",
        "\n",
        "                              ).to(device)\n",
        "\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "id": "qbKUWavQ8q5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "dxIE5fdj_NHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Functionizing training and testing loops"
      ],
      "metadata": {
        "id": "N7dIgElgBIzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, accuracy_fn, device: torch.device = device):\n",
        "  \"\"\"Performs training step with model trying to learn on data_loader\"\"\"\n",
        "  train_loss, train_acc = 0,0\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # Put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true = y, y_pred = y_pred.argmax(dim=1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "        # Print progress only every 400 batches\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss : .5f} | Train acc: {train_acc : .2f} %\")\n"
      ],
      "metadata": {
        "id": "_SwnFBM7BLaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "    \"\"\"Performs a testing loop step on model going over data_loader\"\"\"\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # send to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss accumulatively\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "\n",
        "            # 3. Calculate accuracy\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculate test loss average per batch\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "    # Print out (removing train_loss reference since it's not available here)\n",
        "    print(f\"\\nTest loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "f82WqjcaBI2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n--------\")\n",
        "  train_step(model=model_1, data_loader=train_dataloader, loss_fn=loss_fn, optimizer = optimizer, accuracy_fn = accuracy_fn, device = device)\n",
        "  test_step(model=model_1, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn = accuracy_fn, device = device)\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "\n",
        "total_train_time_model_1 = print_train_time(\n",
        "    start=train_time_start_on_gpu,\n",
        "    end=train_time_end_on_gpu,\n",
        "    device=device # Get the actual device the model is on\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "MR7n--BG9dd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note:** Sometimes, depending on your data/hardware, you might find that your model trains faster on CPU than GPU.\n",
        "?Why is this?\n",
        "\n",
        ">1. It could be that the overhead for copying data/model to and from GPU outweighs the compute benefits offered by the GPU.\n",
        ">2. The hardware you're using has a better CPU in terms of compute capability than the GPU (unlikely).\n"
      ],
      "metadata": {
        "id": "P3YbtZPZGPQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "0VgFclYG9yIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_time_model_0"
      ],
      "metadata": {
        "id": "iMZZedrEE1nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1, data_loader = test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "7Sv8cAKiE23B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Building a Convolutional Neural Network (CNN)\n",
        "\n",
        "CNN's are also known as ConvNets.\n",
        "\n",
        "CNN's are known for their capabilities to find patterns in visual data.\n",
        "\n",
        "More layers = more chance to find patterns in data"
      ],
      "metadata": {
        "id": "hgDs3nE9QbMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "  \"\"\"Model Architecture that replicates the TinyVGG mpdel from the CNN explainer website.\"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2) #takes the max value of the input and outputs it\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2) #takes the max value of the input and outputs it\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units * 7 * 7, # There's a trick to calculating this\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(f\"Output shape of conv_block_1: {x.shape}\")\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(f\"Output shape of conv_block_2: {x.shape}\")\n",
        "    x = self.classifier(x)\n",
        "    # print(f\"Output shape of classifier: {x.shape}\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "GDArNFqDT66C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIyGWZAoN63J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1, #black and white image, so only one colour channel, colour = 3\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)\n",
        "                              ).to(device)\n",
        "\n",
        "model_2"
      ],
      "metadata": {
        "id": "fHVTlV5mRVuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Stepping through `nnConv2d()`\n",
        "\n",
        "https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "Conv2D (Convolution 2D)\n",
        "Imagine you have an image represented as a grid of numbers (pixel values). A Conv2D operation works by:\n",
        "\n",
        "Using a filter (or kernel): This is a small grid of numbers (e.g., 3×3 or 5×5).\n",
        "Sliding this filter: The filter moves across the original image grid, step by step.\n",
        "Performing multiplication: At each position, the filter is placed over a section of the image, and each filter value is multiplied with the corresponding image value underneath it.\n",
        "Summing up: All these multiplications are added together to produce a single number.\n",
        "Building a new image: As the filter slides across the entire image, it creates a new grid of numbers (the \"feature map\").\n",
        "\n",
        "Think of it like shining a flashlight over different parts of an image, where the flashlight highlights certain features (like edges, textures, or shapes) depending on what the filter is designed to detect."
      ],
      "metadata": {
        "id": "LXzSN_ekRVyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "test_image = images[0]\n",
        "\n",
        "print(f\"Image batch shape: {images.shape}\")\n",
        "print(f\"Single image shape: {test_image.shape}\")\n",
        "print(f\"Test image: \\n{test_image}\")"
      ],
      "metadata": {
        "id": "18uZGuYxRV0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a single conv2d layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, # in_channels is the same number of colour channels as the colour input image\n",
        "                       out_channels=10, #equial to the number of hidden layers\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=1\n",
        "                       )\n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_output = conv_layer(test_image)\n",
        "conv_output.shape"
      ],
      "metadata": {
        "id": "z_FGyRf-RV32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Stepping through `nn.MaxPool2d()`\n",
        "\n",
        "MaxPool2D (Max Pooling 2D)\n",
        "After applying convolutions, MaxPool2D helps reduce the size of the feature maps:\n",
        "\n",
        "Defining a window: Usually a small 2×2 grid.\n",
        "Sliding this window: Moving it across the feature map (typically without overlap).\n",
        "Taking the maximum: In each window position, only the largest value is kept.\n",
        "Creating a smaller output: The result is a downsized version of the original feature map.\n",
        "\n",
        "MaxPooling essentially says, \"In this small region, let's just keep the strongest signal and ignore the rest.\" This:\n",
        "\n",
        "Reduces the computational load\n",
        "Makes the network focus on the most important features\n",
        "Helps the network become less sensitive to small shifts in the input image\n",
        "\n",
        "Together, Conv2D layers detect patterns while MaxPool2D layers summarize them and make the network more efficient."
      ],
      "metadata": {
        "id": "POg7UES--oeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.imshow(image.to(device).squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "R0MU63rS_Uu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass image through model\n",
        "rand_image_tensor = torch.randn(size=(1,28,28)).to(device)\n",
        "rand_image_tensor"
      ],
      "metadata": {
        "id": "dAP7tbXfEsyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2(rand_image_tensor.unsqueeze(0))"
      ],
      "metadata": {
        "id": "HCSIYCVRKwWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out image shape without unsqueeze (no need for this anyway it works fine in this version of pytorch)\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(0).shape}\" )\n",
        "\n",
        "# Create a sample nn.MaxPool2d layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going thorugh conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n"
      ],
      "metadata": {
        "id": "bbEcNNDf-rRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create random tensor with a similar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1,1,2,2))\n",
        "print(f\"\\n Random tensor: \\n{random_tensor}\")\n",
        "print(f\"\\n Random tensor shape: \\n{random_tensor.shape}\")\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass random tensor through maxpool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\n Max pool tensor: {max_pool_tensor}\")\n",
        "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n"
      ],
      "metadata": {
        "id": "wKg0tajZArs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Seup a loss function and optimizer for `model_2`"
      ],
      "metadata": {
        "id": "CRq97QDnK0mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up loss function/eval metrics/optimizer\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "V79X9U64K3qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n--------\")\n",
        "  train_step(model=model_2, data_loader=train_dataloader, loss_fn=loss_fn, optimizer = optimizer, accuracy_fn = accuracy_fn, device = device)\n",
        "  test_step(model=model_2, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn = accuracy_fn, device = device)\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "\n",
        "total_train_time_model_2 = print_train_time(\n",
        "    start=train_time_start_on_gpu,\n",
        "    end=train_time_end_on_gpu,\n",
        "    device=device # Get the actual device the model is on\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "HM7a7aPgLI35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results\n",
        "model_2_results = eval_model(\n",
        "    model=model_2, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device\n",
        ")\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "wyvCOoy9MvbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHngU6ohOA-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare model results and training time"
      ],
      "metadata": {
        "id": "_h3_Stc8NNOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\n",
        "\n",
        "compare_results"
      ],
      "metadata": {
        "id": "NYwL5PaDNRC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improve model"
      ],
      "metadata": {
        "id": "H8AxNR03ODzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedFashionMNISTModel(nn.Module):\n",
        "    \"\"\"Improved model architecture for FashionMNIST dataset with batch normalization,\n",
        "    dropout, and residual connections.\"\"\"\n",
        "    def __init__(self, input_shape: int = 1, hidden_units: int = 64, output_shape: int = 10):\n",
        "        super().__init__()\n",
        "\n",
        "        # First convolutional block with batch normalization\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Second convolutional block with increased channels\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units*2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Third convolutional block for more depth\n",
        "        self.conv_block_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units*2, out_channels=hidden_units*4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(hidden_units*4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Global average pooling and classifier with dropout\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),  # Adding dropout for regularization\n",
        "            nn.Linear(in_features=hidden_units*4, out_features=hidden_units*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(in_features=hidden_units*2, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.conv_block_3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-JvmpZqvOEzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_step(model, data_loader, loss_fn, optimizer, accuracy_fn, device):\n",
        "#     \"\"\"Performs a training step for a PyTorch model.\"\"\"\n",
        "#     # Put model in train mode\n",
        "#     model.train()\n",
        "\n",
        "#     # Setup train loss and train accuracy values\n",
        "#     train_loss, train_acc = 0, 0\n",
        "\n",
        "#     # Loop through data loader batches\n",
        "#     for batch, (X, y) in enumerate(data_loader):\n",
        "#         # Send data to target device\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "\n",
        "#         # 1. Forward pass\n",
        "#         y_pred = model(X)\n",
        "\n",
        "#         # 2. Calculate loss\n",
        "#         loss = loss_fn(y_pred, y)\n",
        "#         train_loss += loss.item()\n",
        "\n",
        "#         # 3. Optimizer zero grad\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # 4. Loss backward\n",
        "#         loss.backward()\n",
        "\n",
        "#         # 5. Optimizer step\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Calculate accuracy\n",
        "#         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "#         train_acc += accuracy_fn(y_pred_class, y)\n",
        "\n",
        "#     # Adjust metrics to get average loss and accuracy per batch\n",
        "#     train_loss = train_loss / len(data_loader)\n",
        "#     train_acc = train_acc / len(data_loader)\n",
        "\n",
        "#     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "#     return train_loss, train_acc\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs=10, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    model.to(device)\n",
        "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                          mode='min',\n",
        "                                                          factor=0.1,\n",
        "                                                          patience=3,\n",
        "                                                          verbose=True)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0, 0\n",
        "\n",
        "        for X, y in train_dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            y_pred = model(X)\n",
        "\n",
        "            # Calculate loss and accumulate\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy and accumulate\n",
        "            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "            train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Adjust per batch\n",
        "        train_loss = train_loss / len(train_dataloader)\n",
        "        train_acc = train_acc / len(train_dataloader)\n",
        "\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        test_loss, test_acc = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_dataloader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                test_pred = model(X)\n",
        "\n",
        "                # Calculate loss and accumulate\n",
        "                loss = loss_fn(test_pred, y)\n",
        "                test_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy and accumulate\n",
        "                test_pred_class = torch.argmax(torch.softmax(test_pred, dim=1), dim=1)\n",
        "                test_acc += (test_pred_class == y).sum().item()/len(test_pred)\n",
        "\n",
        "        # Adjust per batch\n",
        "        test_loss = test_loss / len(test_dataloader)\n",
        "        test_acc = test_acc / len(test_dataloader)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        # Store results\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "d6HXDXNqOJi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = ImprovedFashionMNISTModel(input_shape=1, hidden_units=64, output_shape=10)\n",
        "optimizer = torch.optim.Adam(model_3.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Z_jg40P0OOb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_time_start_model_3 = timer()\n",
        "\n",
        "model_3_results = train_model(model_3, train_dataloader, test_dataloader, optimizer, loss_fn, epochs=3)\n",
        "\n",
        "train_time_end_model_3 = timer()\n",
        "\n",
        "total_train_time_model_3 = print_train_time(\n",
        "    start=train_time_start_on_gpu,\n",
        "    end=train_time_end_on_gpu,\n",
        "    device=device # Get the actual device the model is on\n",
        ")"
      ],
      "metadata": {
        "id": "5qOiBbmbOWQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "model_3_results = eval_model(\n",
        "    model=model_3, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device\n",
        ")\n",
        "compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results, model_3_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "JjJqREnl2wol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training time to results comparison\n",
        "compare_results[\"training_time\"] = [total_train_time_model_0, total_train_time_model_1, total_train_time_model_2, total_train_time_model_3]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "a3ig3yin2wx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accuracy / %\")\n",
        "plt.ylabel(\"model\")"
      ],
      "metadata": {
        "id": "5zY8qk6J2w2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Make and evaluate random predictions with the best model\n"
      ],
      "metadata": {
        "id": "hsyej2wD2w5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "  pred_probs = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for sample in data:\n",
        "      # Prepare sample (add a batch dimension and pass to target device)\n",
        "      sample = torch.unsqueeze(sample, dim = 0).to(device)\n",
        "\n",
        "      # Forward pass (model outputs raw logits)\n",
        "      pred_logit = model(sample)\n",
        "\n",
        "      # Get prediction probability (logit to prediction probability)\n",
        "      pred_prob = torch.softmax(pred_logit.squeeze(), dim = 0)\n",
        "\n",
        "      # Get pred_prob off GPU and on CPU for further calculations\n",
        "      pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "  # Stack the pred_probs to turn list into tensor\n",
        "\n",
        "  return torch.stack(pred_probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "0REGqsz1ZY0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "  test_samples.append(sample)\n",
        "  test_labels.append(label)\n",
        "\n",
        "# View the first sample shape\n",
        "test_samples[0].shape\n"
      ],
      "metadata": {
        "id": "YFcaXzG62w9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_samples[0].squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[test_labels[0]])"
      ],
      "metadata": {
        "id": "Znevw3lI2v0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "pred_probs = make_predictions(model=model_3, data=test_samples)\n",
        "\n",
        "# View first two prediction probabilities\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "RqCBzmz6a4w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we convert prediction_probs into pred labels to compare with test_labels\n",
        "\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "zcBWSw8obPp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9,9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  #plot target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # fi|nd pred labels in text form\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # get truth label\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # create title\n",
        "  title_text = f\" Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # check equality\n",
        "  if pred_label == truth_label:\n",
        "    plt.title(title_text, fontsize=10, c=\"g\")\n",
        "  else:\n",
        "    plt.title(title_text, fontsize=10, c=\"r\")\n",
        "\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "lQHTIJndbkic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Making a confusion matrix for further prediction evaluation\n",
        "\n",
        "\n",
        "A confusion matric is a great way to evaluate your classification model visually.\n",
        "\n",
        "1. Make predictions with our trained model with the test dataset\n",
        "2. Make confusion matrix `torchmetrics.ConfusionMatrix`\n",
        "3. Plot the confusion matrix using `mlxtend.plotting.plot_confusion_matrix`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZdlqhCvitvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make predictions with trained model\n",
        "\n",
        "y_preds = []\n",
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions...\"):\n",
        "    # send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    #Do forward pass\n",
        "    y_logit = model_3(X)\n",
        "\n",
        "    #Turn predictions from logits to prediction probs -> prediction labels\n",
        "    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "\n",
        "\n",
        "# concac list of preds into tensor\n",
        "# print(y_preds)\n",
        "y_pred_tensor = torch.cat(y_preds)\n",
        "y_pred_tensor\n"
      ],
      "metadata": {
        "id": "hR-jsC7jkYSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torchmetrics, mlxtend\n",
        "  print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "  assert int(mlxtend.__version__.split(\".\")[1] >= 19, \"mlxtend version should be 0.19.0 or higher\")\n",
        "except:\n",
        "  !pip install torchmetrics -U mlxtend\n",
        "  import torchmetrics, mlxtend\n",
        "  print(f\"mlxtend version: {mlxtend.__version__}\")\n"
      ],
      "metadata": {
        "id": "2hGWdUPhlghy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Set up confusion instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes = len(class_names), task=\"multiclass\")\n",
        "confmat_tensor = confmat(preds = y_pred_tensor, target=test_data.targets)\n",
        "\n",
        "# 3.  plot confmat\n",
        "\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat = confmat_tensor.numpy(), # matplotlib needs numpy\n",
        "    class_names = class_names,\n",
        "    figsize=(10, 7)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "adEo4Rb9kSPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iHrcHnwfokER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Save and load our best model\n"
      ],
      "metadata": {
        "id": "bP9bSbUQkTWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"03_pytorch_computer_vision_model_3.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "MODEL_SAVE_PATH\n",
        "\n",
        "# Savemodel state dict\n",
        "\n",
        "print(f\"Saving model to {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj = model_3.state_dict(), f=MODEL_SAVE_PATH) #state_dict is our model's learned parameters on the dataset (weights, biases etc)\n"
      ],
      "metadata": {
        "id": "tQeXlVrSn2B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and create model 3 instance from saves model\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_3 = ImprovedFashionMNISTModel(input_shape = 1, hidden_units = 64, output_shape = len(class_names))\n",
        "\n",
        "#load in saved state dict\n",
        "loaded_model_3.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to target device\n",
        "loaded_model_3.to(device)\n"
      ],
      "metadata": {
        "id": "5pUji_HooGex"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}