{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJZDZkIw/Bgum7EuyFPCvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viv-bad/pytorch-course/blob/master/03_pytorch_computer_vision_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Computer vision libraries in PyTorch\n",
        "\n",
        "* `torchvision` - base/main computer vision library in PyTorch\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computer vision\n",
        "* `torchvision.models` - pretrained computer vision models that you can leverage for your own problems\n",
        "* `torchvision.transforms` - functions for manipulating your vision data (images) to be suitable for use with an ML model.\n",
        "* `torch.utils.data.Dataset` - to create our own dataset with out own data, we have the base class for this in PyTorch\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1ffNp_vdGWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print (torch.__version__)\n",
        "print (torchvision.__version__)\n",
        "\n",
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "bFXrjZ_EkkFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", #where to download data to\n",
        "    train=True, # do we want the training dataset/testing dataset\n",
        "    download=True, #do we want to download it\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None, #do we want to transform the target data/labels\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\", #where to download data to\n",
        "    train=False, # do we want the training dataset/testing dataset\n",
        "    download=True, #do we want to download it\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None, #do we want to transform the target data/labels\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "00glpzCYmjtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "PvuJFRa5pr8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training example\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "PijyoxLgpv_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "PstsPXwop2ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx #we have 10 different items of clothing, so output shape must be 10"
      ],
      "metadata": {
        "id": "MA9iwOTAqE-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "EJz3-vLsqNWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our image\n",
        "print(f\"Image shape: {image.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Image label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "OeZ7uTLXqSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualising our data"
      ],
      "metadata": {
        "id": "R6aIVConqZgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze()) # matplotlib expects [height, width, colour_channel], so squeeze to get rid of colour channel dimension\n",
        "plt.title(f\"{label},  {class_names[label]}\")"
      ],
      "metadata": {
        "id": "uiZXnrM1rEw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"{label},  {class_names[label]}\")\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "rQfCwlwLrE1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "rows, cols = 4,4\n",
        "for i in range(1, rows*cols+1):\n",
        "  random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "  img, label = train_data[random_idx]\n",
        "  fig.add_subplot(rows, cols, i)\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.title(f\"{label},  {class_names[label]}\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "i4eeInkirE4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think these items of clothing (images) could be modelled with pure linear lines? Or do we need non linearity"
      ],
      "metadata": {
        "id": "z9vnqlyirE7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our dataset into a Python iterable.\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches).\n",
        "\n",
        "Why?\n",
        "\n",
        "1. It is more computationally efficient, as in your comp hardware may not be able to look (store in memory) all the images in one hit. So break it down to 32 images at one time (batch_size=32).\n",
        "\n",
        "2. It gives our neural network more chances to update its gradients per epoch.\n"
      ],
      "metadata": {
        "id": "ZCnRlsJ-tRDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train dataset into DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # shuffle training data to remove order, so training data doesn't have order\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)# don't shuffle test/eval data, it's good to have the data in the same order. our model will never actually see the test data during training anyway\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "vlk0F6LbtZc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train_dataloader: {len(train_dataloader)}, batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test_dataloader: {len(test_dataloader)}, batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "9GBTkjQMtYu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside train dataloader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "hOirDmKUtY08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "kOYNARijtYxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of ML. modelling experiments, always start with a baseline model.\n",
        "\n",
        "A baseline model is a simple model you will try to improve upon with subsequent models/experiments.\n",
        "\n",
        "Start simple and add complexity when necessary."
      ],
      "metadata": {
        "id": "mc6yQh-JtRGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "x.shape\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [colour_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [colour_channels, height*width]\") # now after flattening we have one value per pixel (height*width)\n"
      ],
      "metadata": {
        "id": "Ryc7dSMctRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "YOaxApUerE-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set up model with input params\n",
        "\n",
        "model_0 = FashionMNISTModelV0(input_shape=784, #28x28\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names) #one for every class\n",
        "\n",
        "                              ).to(device)\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "tq7X7yd54Zkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1,1,28,28]).to(device)\n",
        "model_0(dummy_x) # output of logits"
      ],
      "metadata": {
        "id": "V6DFWPbJ4Z4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "UawCH-YK4Z7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function - since we're working with multiclass data, our loss function is `nn.CrossEntropyLoss()`\n",
        "* Optimizer - we will stick with `torch.optim.SGD()` (stochastic gradient descent)\n",
        "* Evaluation metric - since we're working on a classification problem, let's use accuracy as our evaluation metric.\n"
      ],
      "metadata": {
        "id": "yn_3sn1a4Z-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "nCJ9WC8B4aAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "Qh593ixg7Mm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a function to time our experiments\n",
        "\n",
        "Machine learning is very experimental.\n",
        "\n",
        "Two main things to track are:\n",
        "\n",
        "1. Model's performance (loss, accuracy etc)\n",
        "2. How fast it runs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENkqtY0a7Q34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start: float, end: float, device: torch.device = None):\n",
        "  \"\"\"\n",
        "  Prints difference between start and end time\"\"\"\n",
        "\n",
        "  total_time = end-start\n",
        "\n",
        "  print(f\"Train time on {device}: {total_time: .3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "9c0mKmE17RkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print_train_time(start=start_time, end=end_time, device=device)\n"
      ],
      "metadata": {
        "id": "BJH_3aGo7613"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creating a training loop and training a model on batches of data\n",
        "\n",
        "1. Loop through epochs\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate test loss *per batch*\n",
        "4. Print out what's happening\n",
        "5. Time is all"
      ],
      "metadata": {
        "id": "SgyYUVVP8Xpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for a progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# set seed and start timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set number of epochs (we'll keep it small for faster training time)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-----\")\n",
        "  ### Training\n",
        "  train_loss = 0\n",
        "  # Add loop to loop thorugh training batches\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model_0(X)\n",
        "\n",
        "    # 2. Calculate loss (per batch)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # accumulate train loss\n",
        "\n",
        "    # 3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4.  Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress only every 400 batches\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  ### Testing\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "      # 1. forward pass - FIXED: Using X_test instead of X\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. Calculate loss accumulatively\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "      # 3. Calculate accuracy\n",
        "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculate test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  # Print out\n",
        "  print(f\"\\nTrain loss: {train_loss:.4f} | Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
        "\n",
        "# Calculate training time - FIXED: Made device agnostic\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(\n",
        "    start=train_time_start_on_cpu,\n",
        "    end=train_time_end_on_cpu,\n",
        "    device=next(model_0.parameters()).device  # Get the actual device the model is on\n",
        ")"
      ],
      "metadata": {
        "id": "QUPY2Uz_8qsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Make predictions and get Model 0 results\n"
      ],
      "metadata": {
        "id": "CSFSic-jBOB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn, device):\n",
        "  \"\"\"Returns a dict containing the results of model predicting on data_loader\"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "\n",
        "  # Handle both tensor and float types appropriately\n",
        "  result = {\"model_name\": model.__class__.__name__}\n",
        "\n",
        "  # Check if loss is a tensor or float\n",
        "  if isinstance(loss, torch.Tensor):\n",
        "    result[\"model_loss\"] = loss.item()\n",
        "  else:\n",
        "    result[\"model_loss\"] = loss\n",
        "\n",
        "  # Check if acc is a tensor or float\n",
        "  if isinstance(acc, torch.Tensor):\n",
        "    result[\"model_acc\"] = acc.item()\n",
        "  else:\n",
        "    result[\"model_acc\"] = acc\n",
        "\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "zak-EMKxCzco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate model - results on test dataset\n",
        "model_0_results = eval_model(model=model_0, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device)\n",
        "\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "pKkbbTGSH0zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "device"
      ],
      "metadata": {
        "id": "yGwh9qJx8qzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modal 1: Building a better model with non-linearity\n",
        "\n",
        "We learned about the power of non-linearity in notebook 2."
      ],
      "metadata": {
        "id": "q2EYR4Nb8qv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "zhzLa1Oi8q2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_1 = FashionMNISTModelV1(input_shape=784, #28x28\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names) #one for every class\n",
        "\n",
        "                              ).to(device)\n",
        "\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "id": "qbKUWavQ8q5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "dxIE5fdj_NHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Functionizing training and testing loops"
      ],
      "metadata": {
        "id": "N7dIgElgBIzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, accuracy_fn, device: torch.device = device):\n",
        "  \"\"\"Performs training step with model trying to learn on data_loader\"\"\"\n",
        "  train_loss, train_acc = 0,0\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # Put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true = y, y_pred = y_pred.argmax(dim=1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "        # Print progress only every 400 batches\n",
        "    if batch % 400 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Divide total train loss by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "\n",
        "  print(f\"Train loss: {train_loss : .5f} | Train acc: {train_acc : .2f} %\")\n"
      ],
      "metadata": {
        "id": "_SwnFBM7BLaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "    \"\"\"Performs a testing loop step on model going over data_loader\"\"\"\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "            # send to device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. forward pass\n",
        "            test_pred = model(X)\n",
        "\n",
        "            # 2. Calculate loss accumulatively\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "\n",
        "            # 3. Calculate accuracy\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculate test loss average per batch\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "    # Print out (removing train_loss reference since it's not available here)\n",
        "    print(f\"\\nTest loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "f82WqjcaBI2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n--------\")\n",
        "  train_step(model=model_1, data_loader=train_dataloader, loss_fn=loss_fn, optimizer = optimizer, accuracy_fn = accuracy_fn, device = device)\n",
        "  test_step(model=model_1, data_loader=test_dataloader, loss_fn=loss_fn, accuracy_fn = accuracy_fn, device = device)\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "\n",
        "total_train_time_model_1 = print_train_time(\n",
        "    start=train_time_start_on_gpu,\n",
        "    end=train_time_end_on_gpu,\n",
        "    device=device # Get the actual device the model is on\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "MR7n--BG9dd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**Note:** Sometimes, depending on your data/hardware, you might find that your model trains faster on CPU than GPU.\n",
        "?Why is this?\n",
        "\n",
        ">1. It could be that the overhead for copying data/model to and from GPU outweighs the compute benefits offered by the GPU.\n",
        ">2. The hardware you're using has a better CPU in terms of compute capability than the GPU (unlikely).\n"
      ],
      "metadata": {
        "id": "P3YbtZPZGPQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "0VgFclYG9yIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_time_model_0"
      ],
      "metadata": {
        "id": "iMZZedrEE1nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1, data_loader = test_dataloader, loss_fn=loss_fn, accuracy_fn=accuracy_fn, device=device)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "7Sv8cAKiE23B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Building a Convolutional Neural Network (CNN)\n",
        "\n",
        "CNN's are also known as ConvNets.\n",
        "\n",
        "CNN's are known for their capabilities to find patterns in visual data.\n",
        "\n",
        "More layers = more chance to find patterns in data"
      ],
      "metadata": {
        "id": "hgDs3nE9QbMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "  \"\"\"Model Architecture that replicates the TinyVGG mpdel from the CNN explainer website.\"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2) #takes the max value of the input and outputs it\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2) #takes the max value of the input and outputs it\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units, # There's a trick to calculating this\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "GDArNFqDT66C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1, #black and white image, so only one colour channel, colour = 3\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)\n",
        "                              ).to(device)\n",
        "\n",
        "model_2"
      ],
      "metadata": {
        "id": "fHVTlV5mRVuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Stepping through `nnConv2d()`\n",
        "\n",
        "https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "Conv2D (Convolution 2D)\n",
        "Imagine you have an image represented as a grid of numbers (pixel values). A Conv2D operation works by:\n",
        "\n",
        "Using a filter (or kernel): This is a small grid of numbers (e.g., 3×3 or 5×5).\n",
        "Sliding this filter: The filter moves across the original image grid, step by step.\n",
        "Performing multiplication: At each position, the filter is placed over a section of the image, and each filter value is multiplied with the corresponding image value underneath it.\n",
        "Summing up: All these multiplications are added together to produce a single number.\n",
        "Building a new image: As the filter slides across the entire image, it creates a new grid of numbers (the \"feature map\").\n",
        "\n",
        "Think of it like shining a flashlight over different parts of an image, where the flashlight highlights certain features (like edges, textures, or shapes) depending on what the filter is designed to detect."
      ],
      "metadata": {
        "id": "LXzSN_ekRVyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "test_image = images[0]\n",
        "\n",
        "print(f\"Image batch shape: {images.shape}\")\n",
        "print(f\"Single image shape: {test_image.shape}\")\n",
        "print(f\"Test image: \\n{test_image}\")"
      ],
      "metadata": {
        "id": "18uZGuYxRV0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a single conv2d layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, # in_channels is the same number of colour channels as the colour input image\n",
        "                       out_channels=10, #equial to the number of hidden layers\n",
        "                       kernel_size=3,\n",
        "                       stride=1,\n",
        "                       padding=1\n",
        "                       )\n",
        "\n",
        "# Pass the data through the convolutional layer\n",
        "conv_output = conv_layer(test_image)\n",
        "conv_output.shape"
      ],
      "metadata": {
        "id": "z_FGyRf-RV32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Stepping through `nn.MaxPool2d()`\n",
        "\n",
        "MaxPool2D (Max Pooling 2D)\n",
        "After applying convolutions, MaxPool2D helps reduce the size of the feature maps:\n",
        "\n",
        "Defining a window: Usually a small 2×2 grid.\n",
        "Sliding this window: Moving it across the feature map (typically without overlap).\n",
        "Taking the maximum: In each window position, only the largest value is kept.\n",
        "Creating a smaller output: The result is a downsized version of the original feature map.\n",
        "\n",
        "MaxPooling essentially says, \"In this small region, let's just keep the strongest signal and ignore the rest.\" This:\n",
        "\n",
        "Reduces the computational load\n",
        "Makes the network focus on the most important features\n",
        "Helps the network become less sensitive to small shifts in the input image\n",
        "\n",
        "Together, Conv2D layers detect patterns while MaxPool2D layers summarize them and make the network more efficient."
      ],
      "metadata": {
        "id": "POg7UES--oeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "R0MU63rS_Uu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass image through model\n",
        "device\n",
        "image.to(device)\n",
        "model_2(image)"
      ],
      "metadata": {
        "id": "dAP7tbXfEsyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out image shape without unsqueeze (no need for this anyway it works fine in this version of pytorch)\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(0).shape}\" )\n",
        "\n",
        "# Create a sample nn.MaxPool2d layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(0))\n",
        "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Shape after going thorugh conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n"
      ],
      "metadata": {
        "id": "bbEcNNDf-rRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create random tensor with a similar number of dimensions to our images\n",
        "random_tensor = torch.randn(size=(1,1,2,2))\n",
        "print(f\"\\n Random tensor: \\n{random_tensor}\")\n",
        "print(f\"\\n Random tensor shape: \\n{random_tensor.shape}\")\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass random tensor through maxpool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "print(f\"\\n Max pool tensor: {max_pool_tensor}\")\n",
        "print(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n"
      ],
      "metadata": {
        "id": "wKg0tajZArs8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}