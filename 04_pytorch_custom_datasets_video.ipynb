{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXU8ibshillJOGNWAPWrRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viv-bad/pytorch-course/blob/master/04_pytorch_custom_datasets_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04. PyTorch Custom Datasets Video Notebook\n",
        "\n",
        "We've used some datasets with PyTorch before, but how do we get our own data into PyTorch?\n",
        "\n",
        "One of the ways to do so is via: custom datasets.\n",
        "\n",
        "## Domain libraries\n",
        "\n",
        "Depending on what you're working on, vision, text, audio, recommendation etc you want to look into each PyTorch domain libraries for existing data loading functions and customisable data loading functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "20E6wLMf46B-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Importing PyTorch and setting up device agnostic code"
      ],
      "metadata": {
        "id": "6QEJ02pU9Brq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "4GfUBQLS84ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device agnostic code\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "l98ul1ud840D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data\n",
        "\n",
        "We will use the food 101 dataset from PyTorch. (101 classes of food, 1000 images per class (75% train 25% test)\n",
        "\n",
        "We will use a smaller subset of this database (3 classes of food and 10% of images)\n",
        "\n",
        "When starting out ML projects, start small then increase the scale when necessary.\n",
        "\n",
        "This speeds up how fast you can experiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "J4a-Cg0C843-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to datafolder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare..\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory already exists... skipping download\")\n",
        "else:\n",
        "  print(f\"{image_path} does not exist... creating one\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak sushi data\n",
        "with open(data_path/\"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloading data...\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping data...\")\n",
        "  zip_ref.extractall(image_path)\n"
      ],
      "metadata": {
        "id": "3OqTYLlJ-hiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Becoming one with the data (data preparation and exploration)\n",
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"Walks through dir_path returning its contents\"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "M0eLOM7v-RE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(image_path)"
      ],
      "metadata": {
        "id": "APARKLs2847N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "qreybhhU84-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Visualise an image\n",
        "\n",
        "\n",
        "Let's write some code to:\n",
        "\n",
        "1. Get all of the miage paths\n",
        "2. Pick random image path using Python's `random.choice()`\n",
        "3. Get the image class name `pathlib.Path.parent.stem`\n",
        "4. Since we are working with images, let's open them with Python's `PIL`\n",
        "5. We'll then show the image and print metadata"
      ],
      "metadata": {
        "id": "ZDwyxDtR85Ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# random.seed(42)\n",
        "\n",
        "# 1. Get all image paths\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "# 2. Pick random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "random_image_path\n",
        "\n",
        "# 3. Get image class from path name (name of directory)\n",
        "image_class = random_image_path.parent.stem\n",
        "print(image_class)\n",
        "\n",
        "# 4. Open Image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "P2WFLC3ws32J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tuen the image into an array\n",
        "img_as_array = np.asarray(img)\n",
        "\n",
        "# Plot image\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(img_as_array)\n",
        "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, colour_channels]\")\n",
        "plt.axis(False)"
      ],
      "metadata": {
        "id": "ltrqH7OI85EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Transforming data\n",
        "\n",
        "Before we can use our image data with PyTorch:\n",
        "\n",
        "1. turn target data into tensors (a numerical representation of images)\n",
        "2. Turn it into a `torch.utils.data.Dataset` and subsequently a `torch.utils.data.DataLoader` to create an interable/batched version of our data. These will be called `Dataset` and `DataLoader`.\n",
        "\n"
      ],
      "metadata": {
        "id": "6H375JsJ85HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n"
      ],
      "metadata": {
        "id": "qyi3Izq29grJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Transforming data with `torchvision.transforms`\n",
        "\n",
        "Transforms help you get your images ready to be used with a model/perform data augmentation"
      ],
      "metadata": {
        "id": "fEdwcpM185Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a transform for image\n",
        "data_transform = transforms.Compose([\n",
        "    #Resize our images to 64x64 - allows use of tinyVG model\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    #Flip images randomly on horizontal axis (increases diversity of dataset)\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # Turn image into torch tensor\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "Je9mZLDB85Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform(img)"
      ],
      "metadata": {
        "id": "H_I2ZnVh85PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_transformed_images(image_paths, transform, n= 3, seed=None):\n",
        "  \"\"\"\n",
        "  Selects random images from path of images and loads/transforms them, then plots the original vs the transformed version.\n",
        "  \"\"\"\n",
        "\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "\n",
        "  random_image_paths = random.sample(image_paths, k=n)\n",
        "  for image_path in random_image_paths:\n",
        "    with Image.open(image_path) as f:\n",
        "      fig, ax = plt.subplots(1,2)\n",
        "      ax[0].imshow(f)\n",
        "      ax[0].set_title(f\"Original\\nSize: {f.size}\")\n",
        "      ax[0].axis(False)\n",
        "\n",
        "      # Transform and plot target image\n",
        "      transformed_f = transform(f).permute(1,2,0)\n",
        "      ax[1].imshow(transformed_f)\n",
        "      ax[1].set_title(f\"Transformed\\nShape: {transformed_f.shape}\")\n",
        "      ax[1].axis(False)\n",
        "\n",
        "      fig.suptitle(f\"Class {image_path.parent.stem}\", fontsize=16)\n",
        "\n",
        "\n",
        "plot_transformed_images(image_path_list, data_transform, n=3, seed=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pqs0jtoc85SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Option 1: Loading image data using `ImageFolder`\n",
        "\n",
        "We can load image classification data using `torchvision.datasets.ImageFolder`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PffAADM785Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Imagefolder to create dataset(s)\n",
        "\n",
        "from torchvision import datasets\n",
        "\n",
        "train_data = datasets.ImageFolder(root=train_dir, transform=data_transform, target_transform = None)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
        "\n",
        "train_data, test_data\n",
        "\n"
      ],
      "metadata": {
        "id": "wDcojnvRql3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names as list\n",
        "class_names = train_data.classes"
      ],
      "metadata": {
        "id": "TnpYbNXgrTrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names as dict\n",
        "class_dict = train_data.class_to_idx"
      ],
      "metadata": {
        "id": "t2cTxaqCrW8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check lengths of our dataset\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "qKr2wAjsrgsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index on the train_data dataset to get a single image and label\n",
        "\n",
        "img, label = train_data[0][0], train_data[0][1]\n",
        "print(f\"Image tensor:\\n {img}\")\n",
        "print(f\"Image shape:\\n {img.shape}\")\n",
        "print(f\"Image datatype:\\n {img.dtype}\")\n",
        "print(f\"Image label:\\n {label}\")\n",
        "print(f\"Image label datatype:\\n {type(label)}\")"
      ],
      "metadata": {
        "id": "fQ0mbd8xrjka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rearrange order of dimensions\n",
        "img_permute= img.permute(1,2,0)\n",
        "\n",
        "# Print out different shapes\n",
        "print(f\"Original shape: {img.shape} -> [colour_channels, height, width]\")\n",
        "print(f\"Original shape: {img_permute.shape} -> [height, width, colour_channels]\")\n",
        "\n",
        "#Plot the image\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.imshow(img_permute)\n",
        "plt.axis(\"off\")\n",
        "plt.title(class_names[label], fontsize=14)"
      ],
      "metadata": {
        "id": "l26HqI5dwFtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Turn loaded images into `DataLoader`'s\n",
        "\n",
        "A `DataLoader` is going to help us turn our `Dataset`'s into iterables and we can customise the `batch_size` so our model can see `batch_size` images at a time."
      ],
      "metadata": {
        "id": "hlQUt8wExD55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 1\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=1, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=1, shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "Gh-5b2N7wn3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader), len(test_dataloader)"
      ],
      "metadata": {
        "id": "8CTy8SZSx-Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "# Batch size will now be 1, can change\n",
        "print(f\"Image shape: {img.shape} ->. [batch_size, color channels, height, width]\")\n",
        "print(f\" Label shape: {label.shape}\")"
      ],
      "metadata": {
        "id": "_0pt94sYyGOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Option 2: Loading ImageData with a Custom Dataset\n",
        "\n",
        "1. Want to be able to load images from file\n",
        "2. Want to be able to get class names from Dataset\n",
        "3. Want to get classes as dictionary from Dataset\n",
        "\n",
        "Pros:\n",
        "* Can create a `Dataset` out of almost anything\n",
        "* Not limited to PyTorch pre-built `Dataset` functions\n",
        "\n",
        "Cons:\n",
        "* Even though you could create `Dataset` out of almost anything, it doesn't mean it will work\n",
        "* Using a custom `Dataset` requires more code, which could be prone to errors or performance issues\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pZ-uvLaJdyhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset # Base Dataset class\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List\n",
        "\n"
      ],
      "metadata": {
        "id": "6l8nxoKLeY1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instrance of torchvision.datasets.Imagefolder()\n",
        "\n",
        "train_data.classes, train_data.class_to_idx"
      ],
      "metadata": {
        "id": "1g4rZTQnemlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want a helper function to return the data like above ^\n",
        "\n"
      ],
      "metadata": {
        "id": "siVRWbA_eq88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Creating a helper function to get class names\n",
        "\n",
        "We want a function to:\n",
        "\n",
        "1. Get the class names using `os.scandir()` to traverse a target dir and the dir is in standard image classification format (train/test, images in class dirs).\n",
        "2. Raise an error if the class names aren't found. (i.e. something wrong with dir structure)\n",
        "3. Turn class names into a dict and list and return them\n",
        "\n"
      ],
      "metadata": {
        "id": "frmBgUrWe_Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup path for target directory\n",
        "target_directory = train_dir\n",
        "print(f\"Target dir: {target_directory}\")\n",
        "\n",
        "# Get the class names from the target directory\n",
        "class_names_found = sorted([entry.name for entry in list(os.scandir(target_directory))])\n",
        "class_names_found"
      ],
      "metadata": {
        "id": "TIkHFpucfT0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
        "  \"\"\"Given a target directory, finds the class folder names\"\"\"\n",
        "  # 1. Get class names by scanning target directory\n",
        "  classes = sorted([entry.name for entry in list(os.scandir(directory)) if entry.is_dir])\n",
        "\n",
        "  # 2. Raise error if no classes found\n",
        "  if not classes:\n",
        "    raise FileNotFoundError(f\"Couldn't find any classes in {directory}\")\n",
        "\n",
        "  # 3. Turn class names to dict idx of index labels\n",
        "  class_to_idx = {class_name: i for i, class_name in enumerate(classes)}\n",
        "  return classes, class_to_idx\n"
      ],
      "metadata": {
        "id": "DndzgBYbfaxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_classes(target_directory)"
      ],
      "metadata": {
        "id": "UjydSd9_gKJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Create a custom `Dataset` to replicate `ImageFolder`\n",
        "\n",
        "To create our own custom dataset, we want toÑ\n",
        "\n",
        "\n",
        "1. Subclass `torch.utils.data.Dataset`\n",
        "2. Init our subclass with a target directory (the directory we want to get data from), as well as a transform if we want to transform our data.\n",
        "3. Create several attributes:\n",
        "  * paths - paths of our images\n",
        "  * transform - transform we want to use\n",
        "  * classes - a list of target classes\n",
        "  * class_to_idx - dict of target classes mapped to integer labels\n",
        "4. Create a function to `load_images()`, this function will open an image\n",
        "5. Overwrite `__len()__` method to return the length of the dataset\n",
        "6. Overwrite the `__getitem()__` method to return a given sample when passed an index\n",
        "\n"
      ],
      "metadata": {
        "id": "5MpJbApSgNlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a custom dataset class\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Subclass torch.utils.data.Dataset\n",
        "class ImageFolderCustom(Dataset):\n",
        "  # 2. Init custom dataset\n",
        "  def __init__(self, targ_dir: str, transform=None):\n",
        "    # 3. Create class attributes\n",
        "    # Get all image paths\n",
        "    self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\"))\n",
        "    # Setup transforms\n",
        "    self.transforms = transform\n",
        "    # Create classes and class_to_idx attrs\n",
        "    self.classes, self.class_to_idx = find_classes(targ_dir)\n",
        "\n",
        "  # 4. Load images function\n",
        "  def load_image(self, index: int) -> Image.Image:\n",
        "    \"\"\"Opens an image via a path and returns it\"\"\"\n",
        "    image_path = self.paths[index]\n",
        "    return Image.open(image_path)\n",
        "\n",
        "  # 5. Overwrite __len()__\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"Returns the total number of samples\"\"\"\n",
        "    return len(self.paths)\n",
        "\n",
        "  # 6. Overwrite __getitem()__\n",
        "  def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "    \"\"\"Returns one sample of data, data and label (X, y).\"\"\"\n",
        "    img = self.load_image(index)\n",
        "    class_name = self.paths[index].parent.name #expects path in format data_folder/class_name/image.jpg\n",
        "    class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "    # Transform if necessary\n",
        "    if self.transforms:\n",
        "      return self.transforms(img), class_idx # return data, label (X, y)\n",
        "    else:\n",
        "      return img, class_idx # return untransformed image and label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n3OUWAt8igjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transform\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.Resize(size=(64,64)),\n",
        "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(size=(64,64)),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LoIUZ7TCkjFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test out image folder custom\n",
        "train_data_custom = ImageFolderCustom(targ_dir = train_dir,\n",
        "                                      transform = train_transforms,\n",
        "                                      )\n",
        "test_data_custom = ImageFolderCustom(targ_dir = test_dir,\n",
        "                                      transform = test_transforms,\n",
        "                                      )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v_K4IbVzlFHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom, test_data_custom"
      ],
      "metadata": {
        "id": "mo76r_CIlUpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(train_data_custom)"
      ],
      "metadata": {
        "id": "IdxLN2WrlZIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data), len(test_data_custom)"
      ],
      "metadata": {
        "id": "-iGV9JO4lj4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom.classes"
      ],
      "metadata": {
        "id": "C2T6posTlnCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_custom.class_to_idx"
      ],
      "metadata": {
        "id": "99SXxPOflqBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for equality between original ImageFolder and ImageFolderCustom datasets\n",
        "print(train_data_custom.classes == train_data.classes)\n",
        "print(test_data_custom.classes == test_data.classes)"
      ],
      "metadata": {
        "id": "pNWJ-Np6lrvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Create a function to display random images\n",
        "\n",
        "1. Take in `Dataset` and a number of other params like class names and how many images to visualise\n",
        "2. To prevent the display getting too big, cap to 10\n",
        "3. Set the random seed for reprod\n",
        "4. Get a list of random sample indexes from the target dataset.\n",
        "5. Setup a matplotlib plot.\n",
        "6. Loop through the random sample idxs and plot them with matplotlib.\n",
        "7. Make sure the dimensions of our images line up with matplotlin (HWC)"
      ],
      "metadata": {
        "id": "NJ9OLnmQmYDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a function to take in a dataset\n",
        "def display_random_images(dataset: torch.utils.data.Dataset, classes: List[str] = None, n: int = 10, display_shape: bool = True, seed: int = None):\n",
        "  # 2. Adjust display if n too high\n",
        "  if n > 10:\n",
        "    n = 10\n",
        "    display_shape = False\n",
        "    print(f\"For display, n should be <=10, setting to 10 and remove shape display\")\n",
        "\n",
        "  # 3. set seed\n",
        "  if seed:\n",
        "    random.seed(seed)\n",
        "\n",
        "  # 4. Get random sample idxs\n",
        "  random_samples_idx = random.sample(range(len(dataset)), k = n)\n",
        "\n",
        "  # 5. set up plot\n",
        "  plt.figure(figsize=(16,8))\n",
        "\n",
        "  # 6. Loop through random sample idxs and plot them\n",
        "  for i, targ_sample in enumerate(random_samples_idx):\n",
        "    targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
        "\n",
        "    # 7. Adjust tensor dimensions for plotting\n",
        "    targ_image_adjust = targ_image.permute(1,2,0) # [C,H,W] -> [H,W,C]\n",
        "\n",
        "    # Plot adjusted sampples\n",
        "    plt.subplot(1, n, i+1)\n",
        "    plt.imshow(targ_image_adjust)\n",
        "    plt.axis(False)\n",
        "    if classes:\n",
        "      title = f\"Class: {classes[targ_label]}\"\n",
        "      if display_shape:\n",
        "        title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
        "    plt.title(title)\n"
      ],
      "metadata": {
        "id": "8RdTKcuzmux_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_random_images(train_data, n=5,classes=class_names, seed=None)"
      ],
      "metadata": {
        "id": "4rsH-VXJny1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_random_images(train_data_custom, n=5,classes=class_names, seed=None)"
      ],
      "metadata": {
        "id": "G-qxWHP7oinx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Turn custom loaded images in to `DataLoader`'s"
      ],
      "metadata": {
        "id": "JDhl4f5oowoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader_custom = DataLoader(dataset=train_data_custom, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
        "test_dataloader_custom = DataLoader(dataset=test_data_custom, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
        "\n",
        "train_dataloader_custom, test_dataloader_custom"
      ],
      "metadata": {
        "id": "QkNbolE72olv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image and label from custom data loader\n",
        "img_custom, label_custom = next(iter(train_dataloader_custom))\n",
        "\n",
        "img_custom.shape, label_custom.shape"
      ],
      "metadata": {
        "id": "1-IlPTQW3BiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Other forms of transforms (data augmentation)\n",
        "\n",
        "Data augmentation is the process of artificially adding diversity to your training data.\n",
        "\n",
        "In the case of image data, this may mean applying various image transformations to the training images.\n",
        "\n",
        "This will result in a more generalizable model to unseen data.\n",
        "\n",
        "\n",
        "Let's take a look at one particular type of data aug used to train pytroch vision models to state of the art levels."
      ],
      "metadata": {
        "id": "qzoDkjMQ3hD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at trivial augment...\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.Resize(size=(224,224)),\n",
        "                                      transforms.TrivialAugmentWide(num_magnitude_bins=32), #intensity of augmentation\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                                      transforms.Resize(size=(224,224)),\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i8W2NEZk4KyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GEt all image paths\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "image_path_list[:10]"
      ],
      "metadata": {
        "id": "PkoAll8U6sPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_transformed_images(\n",
        "    image_paths=image_path_list, transform=train_transform, n = 3, seed = None\n",
        ")"
      ],
      "metadata": {
        "id": "vnW52bt06zLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model 0: TinyVGG without data augmentation\n"
      ],
      "metadata": {
        "id": "1jmiw5zd7FgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Creating transforms and loading data for Model 0"
      ],
      "metadata": {
        "id": "WiiY8OaLnV4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simple transform\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "WTyInYxEnkbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and transform data\n",
        "from torchvision import datasets\n",
        "train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n",
        "\n",
        "# 2. turn datasets into dataloaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batchsize and num workers\n",
        "BATCH_SIZE=32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader_simple = DataLoader(dataset=train_data_simple, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS)\n",
        "\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "-2BufMiKnV8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Create TinyVGG model class"
      ],
      "metadata": {
        "id": "EFZowzwanV_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyVGG(nn.Module):\n",
        "  \"\"\"Model architecture copying TinyVGG from CNN Explainer\"\"\"\n",
        "  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2) # default stride is same as kernel size\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2) # default stride is same as kernel size\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(), # turns convolutional blocks into feature vectors\n",
        "        nn.Linear(in_features=hidden_units * 13 * 13 # this makes the matrices mat1 and mat2 line up after flattening\n",
        "                  , out_features=output_shape)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    # print(x.shape)\n",
        "    return x\n",
        "\n",
        "    # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # speed up GPU computations with operator fusion instead"
      ],
      "metadata": {
        "id": "dbhmiPeJokWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3,# number of color channels in out image data\n",
        "                  hidden_units=10,\n",
        "                  output_shape = len(class_names)\n",
        "\n",
        "                  ).to(device)\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "7ej-9BMPnWB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Try a forward pass on a single image to test the model"
      ],
      "metadata": {
        "id": "nPH39APBr6FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single image batch\n",
        "image_batch, label_batch = next(iter(train_dataloader_simple))\n",
        "image_batch.shape, label_batch.shape"
      ],
      "metadata": {
        "id": "Ux6ruUW4sGOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a forward pass\n",
        "model_0(image_batch.to(device))"
      ],
      "metadata": {
        "id": "sDDJCLR6sOOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 use `torchinfo` to get an idea of the shapes going through our model"
      ],
      "metadata": {
        "id": "vnggJTdYtaKM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8pwWHDatear"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "summary(model_0, input_size=[1,3,64,64]) # example input of a batch of one image [batch_size, colour_channel, height, width]\n",
        "# Summary here mocks a forward pass in the model with the input size you give it\n"
      ],
      "metadata": {
        "id": "hMrHcQ10sqWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 Create train and test loop functions\n",
        "\n",
        "* `train_step()` takes in model and dataloader and trains model on dataloader.\n",
        "* `test_step()` takes in a model and dataloader and evaluates the model on the dataloader."
      ],
      "metadata": {
        "id": "8XlNQMCxtWBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train_step()\n",
        "\n",
        "def train_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, device=device):\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Set up eval metrics\n",
        "  train_loss, train_acc = 0,0\n",
        "\n",
        "  # loop through data loader data batches\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    # send data to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X) # outputs model logits\n",
        "\n",
        "    # 2. Calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. optimiser 0 grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # calculate the accuracy metric\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "\n",
        "    train_acc += (y_pred_class==y).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "IZ3KjLAd_YzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test_step()\n",
        "def test_step(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, device=device):\n",
        "\n",
        "  # put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # set up test loss and test acc\n",
        "  test_loss, test_acc = 0,0\n",
        "\n",
        "  # Tuyrn on inference mode\n",
        "  with torch.inference_mode():\n",
        "    # loop through dataloader batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # send data to the target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. Calculate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # 3. Calculate accuracy\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/ len(test_pred_labels))\n",
        "    # Adjust metrics to get av loss and acc per batch\n",
        "    test_loss = test_loss/ len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "Jk-zLgWQ_YLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6 Creating a `train()` function to combine `train_step()` and `test_step()`"
      ],
      "metadata": {
        "id": "70r2ltR9B1br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "#1. Create train function that takes in various model params etc...\n",
        "def train(model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module = nn.CrossEntropyLoss(), epochs: int = 5, device=device):\n",
        "  \"\"\"Train model\"\"\"\n",
        "\n",
        "  # 2. Create empty results dict\n",
        "  results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "  # 3. Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model, dataloader=train_dataloader, loss_fn=loss_fn, optimizer=optimizer, device=device)\n",
        "    test_loss, test_acc = test_step(model=model, dataloader=test_dataloader, loss_fn=loss_fn, device=device)\n",
        "\n",
        "    # 4. Print out what's happening\n",
        "    print(f\"Epoch: {epoch} | Train loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.4f}\")\n",
        "\n",
        "    # 5. Update results dict\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  # Return stored results\n",
        "  return results"
      ],
      "metadata": {
        "id": "fF6PGLHK_XfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7 Train and evaluate model 0\n"
      ],
      "metadata": {
        "id": "cFZJvDLcB7ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Recreate an instance of TinyVGG\n",
        "model_0 = TinyVGG(input_shape=3, # number of colour channels in images\n",
        "                  hidden_units = 10,\n",
        "                  output_shape = len(train_data.classes)\n",
        "                  )\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params = model_0.parameters(), lr = 0.001)\n",
        "\n",
        "#timers\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "start_time = timer()\n",
        "\n",
        "\n",
        "# train model_0\n",
        "model_0_results = train(model=model_0, train_dataloader = train_dataloader_simple, test_dataloader = test_dataloader_simple, optimizer = optimizer, loss_fn = loss_fn, epochs = NUM_EPOCHS, device=device)\n",
        "\n",
        "# End timer\n",
        "end_time = timer()\n",
        "print(f\"Total training time: {end_time - start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "2IR3xxmUDwt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "X3WXS3IOEqp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.8 Plot the loss curvers of Model 0\n",
        "\n",
        "A **loss curve** is a way of trakcing your model's performance over time\n"
      ],
      "metadata": {
        "id": "voTm5WKHQo8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_0_results_keys\n",
        "model_0_results.keys()"
      ],
      "metadata": {
        "id": "LElj_7CvQz9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(results: Dict[str, List[float]]):\n",
        "  \"\"\"Plots training curves of a results dictionary\"\"\"\n",
        "  # Get loss vaules of the results dictionary (training and test)\n",
        "  loss = results[\"train_loss\"]\n",
        "  test_loss = results[\"test_loss\"]\n",
        "\n",
        "\n",
        "  # Get accuracy\n",
        "  accuracy = results[\"train_acc\"]\n",
        "  test_accuracy = results[\"test_acc\"]\n",
        "\n",
        "  # num epochs\n",
        "  epochs = range(len(results[\"train_loss\"])\n",
        "  )\n",
        "\n",
        "  # setup plit\n",
        "  plt.figure(figsize=(15,7))\n",
        "\n",
        "  # plto loss\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss, label=\"train_loss\")\n",
        "  plt.plot(epochs, test_loss, label=\"test_loss\")\n",
        "  plt.title(\"Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        "  #Plot accuracy\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
        "  plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
        "  plt.title(\"Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Vj4_61MfQzYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_0_results)"
      ],
      "metadata": {
        "id": "-oYc9Md4RuaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. What should an ideal loss curve look like?\n",
        "\n",
        "A loss curve is one of the best ways to troubleshoot a model, we want it to go down overtime, while accuracy goes up over time.\n",
        "\n",
        "https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves"
      ],
      "metadata": {
        "id": "K2lX60OrRyNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Model 1: TinyVGG with data augmentation - helps deal with overfitting (augmentation)\n",
        "\n",
        "- Artificially increases diversity of dataset without getting more data"
      ],
      "metadata": {
        "id": "8Aj5zAGdSUH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1 Create transform with data augmentation\n"
      ],
      "metadata": {
        "id": "F9SYqlTBXL1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training transform\n",
        "from torchvision import transforms\n",
        "train_transform_trivial = transforms.Compose([\n",
        "    transforms.Resize(size=(64, 64)),\n",
        "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform_simple = transforms.Compose([ # never augment the test dataset\n",
        "    transforms.Resize(size=(64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ahsMqdgXixU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Create train and test `Dataset`'s and `DataLoader`'s with data augmentation"
      ],
      "metadata": {
        "id": "M_EtrCHbX7TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn image folders into Datasets\n",
        "from torchvision import datasets\n",
        "train_data_augmented = datasets.ImageFolder(root=train_dir, transform=train_transform_trivial)\n",
        "\n",
        "test_data_simple = datasets.ImageFolder(root=test_dir, transform=test_transform_simple)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d0TLnV19YAeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn datasets into dataloaders\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "train_dataloader_augmented = DataLoader(dataset=train_data_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_dataloader_simple = DataLoader(dataset=test_data_simple, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n"
      ],
      "metadata": {
        "id": "D5VdJyMCYYNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 Construct and train model 1\n",
        "\n",
        "This time we will use the same model architecture, but this time we have augmented the training dataset.\n",
        "\n",
        "Remember, you typically start as simple as possible then iterate and add complexity to improve model performance."
      ],
      "metadata": {
        "id": "Obi_MN2yYqtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model_1 and send it to the target device\n",
        "torch.manual_seed(42)\n",
        "model_1 = TinyVGG(input_shape=3, hidden_units=10, output_shape= len(train_data_augmented.classes)).to(device)\n",
        "model_1"
      ],
      "metadata": {
        "id": "6kYhOQX7bATn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a model and dataloaders, let's make the loss function and optimizer and call `train()` to train and evaluate our model"
      ],
      "metadata": {
        "id": "9ype-wWdbAXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Setup loss and optim\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params = model_1.parameters(), lr=0.001)\n",
        "\n",
        "# Start timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# train model 1\n",
        "model_1_results = train(model=model_1, train_dataloader=train_dataloader_augmented, test_dataloader=test_dataloader_simple, optimizer=optimizer, loss_fn=loss_fn, epochs=NUM_EPOCHS, device=device)\n",
        "\n",
        "end_time = timer()\n",
        "print(f\"Total training time for model_1: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "E-nAP6sEe_V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4 Plot the loss curves od model_1"
      ],
      "metadata": {
        "id": "1CM7V9Q6gCr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "9PcYR8f_bAai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(model_1_results)"
      ],
      "metadata": {
        "id": "FUjcPacObAdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Compare model results\n",
        "\n",
        "After evaluating our modelling experiments compare them\n",
        "\n",
        "There's a few ways to do this:\n",
        "1. Hard coding (what we're doing)\n",
        "2. Tensorboard\n",
        "3. Weights & Biases"
      ],
      "metadata": {
        "id": "1MFOUNZBbAfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "model_0_df = pd.DataFrame(model_0_results)\n",
        "model_1_df = pd.DataFrame(model_1_results)"
      ],
      "metadata": {
        "id": "LlTrqlCGhmQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot model results on same plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "epochs = range(len(model_0_df))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\n",
        "plt.title(\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Train Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\n",
        "plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\n",
        "plt.title(\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "id": "6Nvr7VKAhsDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mL37yneEiLHd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}